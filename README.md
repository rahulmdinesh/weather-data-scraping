# Weather Data Scraping Project

## Overview

A complete data scraping, cleaning, and visualization pipeline for collecting city-level climate data from [climate-data.org](https://climate-data.org), cleaning it into a structured CSV format, and visualizing the results. This project uses:
  - **Selenium 4**: Automating a web browser to load and navigate dynamic pages.

  - **BeautifulSoup**: Parsing and extracting structured data from HTML content.

  - **Pandas**: Cleaning, organizing, and saving scraped data into structured tables.

  - **Streamlit**: Creating an interactive web interface to run and visualize the scraping process.

## Project Structure

The project is organized as follows:

- **`data/`**: Contains the data files generated during the scraping and cleaning process.

  - `cleaned_data.csv`: A cleaned CSV file prodcued by scrips/extract_clean_data.py with weather data for the locations scraped by the scrips/extract_urls.py script.

  - `urls.json`: A json file produced by scrips/extract_urls.py and contains a hierarchical structure of continents, countries, and cities along with their respective URLs.

- **`notebooks/`**: Contains Jupyter notebooks for analysis and visualization.
  - `visualization.ipynb`: A notebook for analyzing and visualizing the extracted and cleaned weather data.

- **`scripts/`**: Contains the Python script used for web scraping and data processing.
  - `extract_urls.py`: Primary scraping to obtain URLs for each city. Exports a json file organized in a nested dictionary format.

  - `extract_clean_data.py`: Secondary scraping to access the URL of each city by loading the json file created earlier and extract data from the weather tables. Exports the cleaned weather data in a csv file.

- **`.gitignore`**: File to specify files and directories that should be ignored by version control.

- **`README.md`**: Project documentation, setup instructions, and usage guide.

- **`requirements.txt`**: List of Python packages and versions required for the project.

## Getting Started

To get started with the project, follow these steps:

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/rahulmdinesh/weather-data-scraping.git
   cd weather-data-scraping
   ```

2. **Install Dependencies:**
    ```bash
    pip install -r requirementes.txt
    ```

3. **Run Scripts and Notebooks:**
    Run the following command to perform the primary scraping and obtain the city URLs. The `data/urls.json` file is generated by this script.
    ```bash
    streamlit run scripts/extract_urls.py
    ```
    
    Run to perform the secondary scraping, extracting and cleaning the weather data. The `data/cleaned_data.csv` file is generated by this script.
    ```bash
    streamlit run scripts/extract_clean_data.py
    ```

    Analyze and visualize the extracted data (`data/cleaned_data.csv`) using `notebooks/visualization.ipynb`.

## Screenshots
- User interface seen after **scripts/extract_urls.py** is finished running:
<img width="1728" height="874" alt="image" src="https://github.com/user-attachments/assets/7b965836-4f26-4b82-bc28-dbcc2acc5258" />
<img width="1728" height="874" alt="image" src="https://github.com/user-attachments/assets/855dca95-4d3a-48fa-bd35-58fa33d37d77" />


- User interface seen after **scripts/extract_clean_data.py** is finished running:
<img width="1728" height="903" alt="image" src="https://github.com/user-attachments/assets/114937b9-06bd-4d1e-a5ab-4dffef9d1ed4" />

- Visualization in Jupyter notebook
<img width="1728" height="818" alt="image" src="https://github.com/user-attachments/assets/5d797a04-17cb-4023-95aa-2ccec56a6072" />
<img width="1728" height="805" alt="image" src="https://github.com/user-attachments/assets/fc16abce-f98e-4df3-a73f-9d281f8a2fed" />
<img width="1728" height="864" alt="image" src="https://github.com/user-attachments/assets/03f06330-2a52-435b-a89b-d3c3029d02ec" />









   
